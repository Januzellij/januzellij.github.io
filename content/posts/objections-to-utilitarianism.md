---
title: "Some Objections to Utilitarianism"
date: 2022-09-11T10:32:12-05:00
draft: false
math: true
---

I want to go over some objections I have with utilitarianism. Partially this is for my own selfish benefit so I can stop thinking about this all the time, but it’s also because most laymen/naive objections to utilitarianism that I’ve encountered are very weak (some examples [here](https://sandhoefner.com/2017/10/27/in-defense-of-utilitarianism/)). This weakness comes from the fact that the way utilitarianism is presented is ingenious, to the point that is seems like the correct ethical system ipso facto; a paragon of rationality floating far above the teeming, grimy hoards of common folk. On the contrary, utilitarianism is ad hoc and relies on massive unjustified assumptions. It is a failure of secular rationality: the rationalist scorns antiquated moral systems, with their unjustified assumptions and myopic view of the world, before embracing an ethical system with far more egregious assumptions and a modern coat of paint.

Let’s look at an example. Suppose I am a utilitarian farmer who’s considering putting pesticides on my crops to get rid of a beetle infestation. To make this decision, I must know how much utility my harvest will give to the people that consume it, the likelihood the pesticides make a difference, and what the worth is of the lives of the insects my pesticides kill. I can then compare these numbers and see what the expected utility is of applying the pesticides.

## Utility makes no sense

Utility, as a concept, makes no sense. There is no reason to believe that there is some number \\( N \\), so that one human life is worth \\( N \\) beetle lives. Sure, you can assign a number, but it’s all garbage. What information is this going to be based upon? What possibly functions as a reasonable explanation here? What would be a convincing argument that, for examples, \\( N \\) should be at least \\( 10,000 \\)?

There are "workarounds" here that seem reasonable, but are just serpentine tricks. For example, someone may ask how much money you’re willing to pay to have/not have an experience, and then build utility based on that. This is a fake question though: the answers aren’t based on anything, they’re random shots in the dark. People do not have the ability, psychologically, to evaluate experiences in this way. It’s analogous to Anselm is making the ontological argument for the existence of God and saying “I can conceive of a supremely perfect being” which he obviously can't, and is deluding himself.s

In this light, it is clearer why the slogan of utilitarianism, “the greatest amount of good” is so deceptive. Whether one says “good”, “well-being”, “happiness”, “utility”, or something else, this word is devoid of meaning. There is no singular entity called “Utility” that captures positive subjective experiences across all agents in a unified way. “Utility” functions as a pseudo-Platonic Form of the Good to marshal wildly different modalities under a common umbrella. Its very existence is a metaphysical axiom of gargantuan proportions.

Not only are experiences incomparable, they aren’t even accessible. To compare the lives of the beetles to the lives of the humans who eat my crops, I need to have a *quantitative measure* of the subjective experience of a beetle. Quantitative is the key word here: there are obviously ways we can infer what the subjective experience of other agents is, otherwise we couldn’t operate as people. However, this requires that subjective experiences are physical entities that can be measured, and I don’t see any reason this should be true.

This topic is covered in more detail in Thomas Nagel’s paper [What is it like to be a bat?](https://warwick.ac.uk/fac/cross_fac/iatl/study/ugmodules/humananimalstudies/lectures/32/nagel_bat.pdf).

The utilitarian may object here that I’m being extreme. Does this mean that we can’t ever know anything about what other entities are thinking or how they are suffering? If so, it seems that people can do harm to any other living thing they want, since we have no idea of their subjective experience, and correspondingly whether they *actually* suffer.

However, I’m not being extreme at all. The implicit assumption here is that the reason we choose to not harm other entities is because we have some sort of knowledge of their subjective experience, but this assumption just isn’t true in day-to-day life. Deontological systems don’t require any knowledge of other beings subjective experience, nor does following your moral instincts. It is a *strength* to ignore subjective experience in this way, not a weakness. Instead, subjective experience can be inferred from other sources, like how another entity communicates or behaves. These non-utilitarian systems take into account subjective experience by lessening the burden of proof: no neuroscience necessary. The key flaw here is not that utilitarianism accounts for subjective experience, but that utilitarian epistemology is unnecessarily restrictive.

The utilitarian may also object to the idea that implementation is part of the utilitarian package. They may claim that utilitarianism merely defines what is moral and immoral, and it is the task of science to determine how best to implement these directives.

This is a cop-out argument for a few reasons. Firstly, the objection to implementation relating to accessibility of subjective experience is not something that can be solved with advancing technology or science: it is a hard philosophical barrier. Secondly, even if I buy this argument, it leaves the utilitarian in a bad position. They now are defending an ethical system that, *by their own admission*, is currently a mere academic exercise, and one that is not able to be put into practice. Finally, utilitarianism without a reasonable implementation is, in practice, deontological: some government or think tank or other institution says that they did the calculations and it just so happens that utilitarianism supports whatever bullshit the institution is currently on about. What a surprise.

Another theoretical problem with utilitarianism is that expected utility is inapproximable. In an expected utility calculation, there are two factors that affect how much a hypothetical situation matters: the probability that it occurs, and the potential effect on total utility. (Technically utilitarians don't have to necessarily use expected utility, but whatever mathematical framework they end up using will depend principally on the same two factors). The problem here is that the potential effect on total utility is arbitrary: the utilitarian now has to give serious credence to any insane hypothetical, as long as the potential effect on utility is high enough.

There are many examples of this: I’ll give a few.

A fairly pedestrian one goes as follows: suppose a government is considering banning farming animals for consumption. Because of the potentially massive effect on total utility, they have to take into consideration the possibility that animals gain substantial utility from being killed and eaten in large numbers. Maybe farm animals are masochists and derive huge pleasure from their execution. Or maybe they’re religious, and it is part of their religion that they are ritually killed for consumption.

A classic instance is a variant on the famous Pascal’s Wager, called [Pascal’s Mugging](https://wiki.lesswrong.com/wiki/Pascal%27s_mugging). One night as you’re walking home, a man steps out of the shadows and demands $5, which you have on you. If you refuse, he will kill 3 billion people in an excruciatingly painful way by using his magic powers. Again, the premise is supposed to be ridiculous: the issue here is that, *merely through his words*, he has forced his way into serious consideration, irrespective of how insane the premise is.

Finally, [here](https://reducing-suffering.org/is-there-suffering-in-fundamental-physics/) is an exhaustive exploration of the question of whether fundamental particles (electrons, quarks, etc) suffer.

The common theme here is that these possibilities are nuts, but there isn't a reliable way to shrink confidence intervals small enough to get a positive outcome. Take the farming example. Suppose the potential pleasure of animals being slaughtered would be \\( 10^{100} \\) utils. It is very unlikely that animals enjoy being slaughtered in this way, but who's to put an estimate on the likelihood? Is the probability less than \\( 10^{-100} \\)? \\( 10^{-1000} \\)? What on earth would this estimation be based on? Because the potential utility is so high, whether the likelihood is \\( 10^{-100} \\) or \\( 10^{-1000} \\) is the difference between a stubbed toe and the Holocaust.

## Practical objections

After all of this nit-picking over epistemology, maybe my biggest problem with utilitarianism is that it is completely alien. It in no way considers the actual psychology of living people, and the things that our brains have been selected over time to do. It is something you invent without any knowledge of human psychology: the creation of ethical principles completely removed from the necessary historical context. The utilitarian treats human brains like computers whose function is to crunch numbers and treat everyone with the same brush of impartiality.

The moral framework that we are equipped with through instincts and moral intuitions is astonishingly effective at accomplishing a specific task: maintaining close social groups in a way that aids the survival of the species. Utilitarianism is designed to "be correct". However, moral systems are bound by sociological measures beyond 'philosophical correctness': they have to inspire everyone to do their best and allow timely and fair resolution of conflicts.

To take a concrete example, utilitarianism dictates that in many cases, wealthier people will be more moral - not because of any actual moral facets of their character, but by the mere fact that they have more discretionary capital available to them.

Suppose John makes $50k a year and Jack makes $60k a year. They both have the same amount of 'hard' expenses (food, rent, car, etc.). Let's say those are $35k a year. They both allocate the remaining $25k the same percentage-wise. As long as the total utility of this discretionary spending is positive, Jack is more moral by utilitarian standards, simply because he has a larger pool to spend.

This kind of conclusion is directly antithetical to social cohesion, and intertwines unpleasantly with the undeniable fact that the average utilitarian is a rich tech worker. Utilitarianism is no stranger to [repugnant conclusions](https://en.wikipedia.org/wiki/Mere_addition_paradox) but it's hard to look at the inevitable consequences of utilitarianism and see not a rational, empirical system for judging moral worth, but yet another institutionalized moral system that attracts adherents who benefit most from it.

*Jesus sat down opposite the place where the offerings were put and watched the crowd putting their money into the temple treasury. Many rich people threw in large amounts. But a poor widow came and put in two very small copper coins, worth only a fraction of a penny. Calling his disciples to him, Jesus said "I tell you the truth, this poor widow has put more into the treasury than all the others. They all gave out of their wealth; but she, out of her poverty, put in everything--all she had to live on". (Mark 12:41-44, NIV)*
